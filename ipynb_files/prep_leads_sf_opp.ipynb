{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a6dc13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from simple_salesforce import Salesforce\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from datetime import date, timedelta\n",
    "from sqlalchemy import create_engine\n",
    "from urllib.parse import quote_plus\n",
    "from functools import lru_cache\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4a6c6252",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "username = os.getenv(\"PG_USERNAME\")\n",
    "password = os.getenv(\"PG_PASSWORD\")\n",
    "host = os.getenv(\"PG_HOST\")\n",
    "port = os.getenv(\"PG_PORT\")\n",
    "database = os.getenv(\"PG_DATABASE\")\n",
    "\n",
    "# URL-encode the password\n",
    "encoded_password = quote_plus(password)\n",
    "\n",
    "engine = create_engine(\n",
    "    f\"postgresql+psycopg2://{username}:{encoded_password}@{host}:{port}/{database}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f176b3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "select distinct id,\n",
    "       reporting_date,\n",
    "       type,\n",
    "       record_type,\n",
    "       name,\n",
    "       owner,\n",
    "       owner_role,\n",
    "       bucket_role,\n",
    "       market_segment,\n",
    "       lead_source, \n",
    "       bucket_lead_source,\n",
    "       state,\n",
    "       intake_year,\n",
    "       intake_month,\n",
    "       cycle,\n",
    "       campus_preference1,\n",
    "       vertical1,\n",
    "       level1,\n",
    "       programme1,\n",
    "       race,\n",
    "       lead_status,\n",
    "       opp_acc_id,\n",
    "       opp_stage,\n",
    "       opp_programme_code,\n",
    "       online_source,\n",
    "       web_source_group,\n",
    "       web_source_grp,\n",
    "       entry_qualification\n",
    "from sf_lead_opp_activity\n",
    "where reporting_date =  %(reporting_date)s;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cfb54dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date, timedelta\n",
    "\n",
    "def get_most_recent_friday() -> date:\n",
    "    \"\"\"\n",
    "    Always return the most recent past Friday (including today if Friday).\n",
    "    Example:\n",
    "      Wed 22 Oct 2025 → Fri 17 Oct 2025\n",
    "      Fri 24 Oct 2025 → Fri 24 Oct 2025\n",
    "      Mon 27 Oct 2025 → Fri 24 Oct 2025\n",
    "    \"\"\"\n",
    "    today = date.today()\n",
    "    # weekday(): Monday=0 ... Sunday=6, so Friday=4\n",
    "    days_since_friday = (today.weekday() - 4) % 7\n",
    "    # subtract number of days since last Friday\n",
    "    return today - timedelta(days=days_since_friday)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "eaff8563",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- usage ---\n",
    "reporting_date = get_most_recent_friday().strftime(\"%Y-%m-%d\")\n",
    "params = {'reporting_date': reporting_date}\n",
    "\n",
    "recent_data = pd.read_sql(query, engine, params=params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3dec756d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load files ---\n",
    "first_tier  = pd.read_excel(r\"C:/Users/112363/OneDrive - Taylor's Education Group/DWH_WIP/programme_code_mapping.xlsx\", sheet_name=\"Tier1\")\n",
    "second_tier = pd.read_excel(r\"C:/Users/112363/OneDrive - Taylor's Education Group/DWH_WIP/programme_code_mapping.xlsx\", sheet_name=\"Tier2\")\n",
    "third_tier  = pd.read_excel(r\"C:/Users/112363/OneDrive - Taylor's Education Group/DWH_WIP/programme_code_mapping.xlsx\", sheet_name=\"Tier3\")\n",
    "fourth_tier = pd.read_excel(r\"C:/Users/112363/OneDrive - Taylor's Education Group/DWH_WIP/programme_code_mapping.xlsx\", sheet_name=\"Tier4\")\n",
    "odl_tier    = pd.read_excel(r\"C:/Users/112363/OneDrive - Taylor's Education Group/DWH_WIP/programme_code_mapping.xlsx\", sheet_name=\"ODL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba6b22a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from functools import lru_cache\n",
    "\n",
    "# -------------------------\n",
    "# 0) Normalize columns (vectorized)\n",
    "# -------------------------\n",
    "def normalize_cols(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df.columns = df.columns.str.strip()\n",
    "    return df\n",
    "\n",
    "first_tier  = normalize_cols(first_tier)\n",
    "second_tier = normalize_cols(second_tier)\n",
    "third_tier  = normalize_cols(third_tier)\n",
    "fourth_tier = normalize_cols(fourth_tier)\n",
    "odl_tier    = normalize_cols(odl_tier)\n",
    "data_df     = normalize_cols(recent_data)\n",
    "\n",
    "# Ensure intake_year column exists & numeric\n",
    "if \"intake_year\" not in data_df.columns:\n",
    "    for alt in [\"Intake Year\", \"intakeyear\", \"IntakeYear\"]:\n",
    "        if alt in data_df.columns:\n",
    "            data_df = data_df.rename(columns={alt: \"intake_year\"})\n",
    "            break\n",
    "if \"intake_year\" in data_df.columns:\n",
    "    data_df[\"intake_year\"] = pd.to_numeric(data_df[\"intake_year\"], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "# Lowercase helpers for consistent text compares\n",
    "for col in [\"programme1\", \"level1\", \"vertical1\"]:\n",
    "    if col in data_df.columns:\n",
    "        data_df[col + \"__lc\"] = data_df[col].astype(str).str.lower()\n",
    "        \n",
    "# -------------------------\n",
    "# 1) Year range parser (vectorized)\n",
    "# -------------------------\n",
    "def prepare_rule_tier(df_rules: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df_rules.copy()\n",
    "    if \"Intake Year\" in df.columns:\n",
    "        # Accept \"YYYY\" or \"YYYY-YYYY\"\n",
    "        yr = df[\"Intake Year\"].astype(str).str.strip()\n",
    "        span = yr.str.extract(r\"^\\s*(\\d{4})\\s*-\\s*(\\d{4})\\s*$\")\n",
    "        single = yr.str.extract(r\"^\\s*(\\d{4})\\s*$\")\n",
    "        df[\"start_year\"] = pd.to_numeric(span[0].fillna(single[0]), errors=\"coerce\")\n",
    "        df[\"end_year\"]   = pd.to_numeric(span[1].fillna(single[0]), errors=\"coerce\")\n",
    "    return df\n",
    "\n",
    "first_tier  = prepare_rule_tier(first_tier)\n",
    "second_tier = prepare_rule_tier(second_tier)\n",
    "third_tier  = prepare_rule_tier(third_tier)\n",
    "fourth_tier = prepare_rule_tier(fourth_tier)\n",
    "odl_tier    = prepare_rule_tier(odl_tier)\n",
    "\n",
    "# -------------------------\n",
    "# 2) Wildmatch helpers (cached + vectorized)\n",
    "# -------------------------\n",
    "@lru_cache(maxsize=2048)\n",
    "def _compile_wildcard(pattern: str):\n",
    "    esc = re.escape(str(pattern))\n",
    "    esc = esc.replace(r\"\\*\", \".*\").replace(r\"\\?\", \".\")\n",
    "    return re.compile(rf\"^{esc}$\", flags=re.IGNORECASE)\n",
    "\n",
    "def series_wildmatch(series: pd.Series, patterns):\n",
    "    \"\"\"Vectorized 'wildmatch' across a Series for many patterns.\"\"\"\n",
    "    s = series.fillna(\"\").astype(str)\n",
    "    # Fast path: combine patterns into one big alternation regex if possible\n",
    "    # (still uses per-pattern compile cache once)\n",
    "    masks = []\n",
    "    for p in patterns:\n",
    "        masks.append(s.str.match(_compile_wildcard(p)))\n",
    "    if not masks:\n",
    "        return pd.Series(False, index=series.index)\n",
    "    # OR all masks\n",
    "    out = masks[0]\n",
    "    for m in masks[1:]:\n",
    "        out = out | m\n",
    "    return out\n",
    "\n",
    "# -------------------------\n",
    "# 3) Compile each Programme_Code_Rule into a vectorized mask\n",
    "# -------------------------\n",
    "_wild_re = re.compile(\n",
    "    r\"wildmatch\\s*\\(\\s*([A-Za-z_][A-Za-z0-9_]*)\\s*,\\s*([^)]+?)\\s*\\)\",\n",
    "    flags=re.IGNORECASE\n",
    ")\n",
    "\n",
    "def _parse_patterns(arg_text: str):\n",
    "    # split comma-separated quoted patterns: 'a','b','c'\n",
    "    # tolerate quotes \" or '\n",
    "    parts = re.findall(r\"\"\"(['\"])(.*?)\\1\"\"\", arg_text)\n",
    "    return [p[1] for p in parts] if parts else []\n",
    "\n",
    "def compile_rule_to_mask(rule_str: str, df: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Turn a Qlik-like rule string into a vectorized boolean pandas Series.\n",
    "    Supports:\n",
    "      - wildmatch(col, 'pat1','pat2',...)\n",
    "      - AND/OR/NOT (case-insensitive)\n",
    "      - simple numeric/string comparisons against df columns (e.g. intake_year < 2022)\n",
    "    \"\"\"\n",
    "    expr = str(rule_str).strip()\n",
    "    expr = re.sub(r\"\\bAND\\b\", \"&\", expr, flags=re.IGNORECASE)\n",
    "    expr = re.sub(r\"\\bOR\\b\",  \"|\", expr, flags=re.IGNORECASE)\n",
    "    expr = re.sub(r\"\\bNOT\\b\", \"~\", expr, flags=re.IGNORECASE)\n",
    "\n",
    "    masks = {}\n",
    "    repls = []\n",
    "    # Replace each wildmatch(...) with a placeholder variable name\n",
    "    for i, m in enumerate(_wild_re.finditer(expr)):\n",
    "        col = m.group(1)\n",
    "        args = m.group(2)\n",
    "        pats = _parse_patterns(args)\n",
    "        placeholder = f\"_WMASK_{i}_\"\n",
    "        repls.append((m.span(), placeholder, col, pats))\n",
    "\n",
    "    # Build the final expression by splicing placeholders in reverse order\n",
    "    expr_list = list(expr)\n",
    "    for (start, end), placeholder, col, pats in reversed(repls):\n",
    "        expr_list[start:end] = placeholder\n",
    "        # choose lowercased column if present to be consistent\n",
    "        col_use = col + \"__lc\" if (col + \"__lc\") in df.columns else col\n",
    "        masks[placeholder] = series_wildmatch(df[col_use] if col_use in df.columns else pd.Series(\"\", index=df.index), pats)\n",
    "\n",
    "    final_expr = \"\".join(expr_list)\n",
    "\n",
    "    # Create evaluation context: df columns as variables\n",
    "    ctx = {c: df[c] for c in df.columns}\n",
    "    ctx.update(masks)\n",
    "\n",
    "    # Evaluate safely with pandas eval (python engine for Series ops)\n",
    "    try:\n",
    "        mask = pd.eval(final_expr, engine=\"python\", local_dict=ctx)\n",
    "        # Ensure boolean Series\n",
    "        mask = mask.astype(bool)\n",
    "    except Exception:\n",
    "        # Fallback: nothing matches if expression fails\n",
    "        mask = pd.Series(False, index=df.index)\n",
    "    return mask\n",
    "\n",
    "# -------------------------\n",
    "# 4) Rule-tier evaluator (vectorized per rule, not per row)\n",
    "# -------------------------\n",
    "def apply_rule_tier(df_rules: pd.DataFrame, df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Returns two Series: name_out, code_out from a rule-based tier.\n",
    "    First match wins (by row order in df_rules).\n",
    "    \"\"\"\n",
    "    name_out = pd.Series(pd.NA, index=df.index, dtype=\"object\")\n",
    "    code_out = pd.Series(pd.NA, index=df.index, dtype=\"object\")\n",
    "\n",
    "    if not {\"Programme_Code_Rule\", \"Programme Name\"}.issubset(df_rules.columns):\n",
    "        return name_out, code_out\n",
    "\n",
    "    # Pre-extract code column existence\n",
    "    has_code = \"Programme Code\" in df_rules.columns\n",
    "\n",
    "    for _, r in df_rules.iterrows():\n",
    "        mask = pd.Series(True, index=df.index)\n",
    "\n",
    "        # Year filter\n",
    "        if \"start_year\" in r and pd.notna(r[\"start_year\"]) and \"intake_year\" in df.columns:\n",
    "            mask &= df[\"intake_year\"].between(int(r[\"start_year\"]), int(r[\"end_year\"]))\n",
    "\n",
    "        # Rule expression\n",
    "        rule = r[\"Programme_Code_Rule\"]\n",
    "        mask &= compile_rule_to_mask(rule, df)\n",
    "\n",
    "        # Only fill where not already set\n",
    "        to_fill = mask & name_out.isna()\n",
    "        if to_fill.any():\n",
    "            name_out.loc[to_fill] = r[\"Programme Name\"]\n",
    "            if has_code and pd.notna(r.get(\"Programme Code\", pd.NA)):\n",
    "                code_out.loc[to_fill] = str(r[\"Programme Code\"])\n",
    "\n",
    "        # Early exit if everything is filled\n",
    "        if name_out.notna().all():\n",
    "            break\n",
    "\n",
    "    return name_out, code_out\n",
    "\n",
    "# -------------------------\n",
    "# 5) Mapping-tier evaluator (vectorized via maps/merges)\n",
    "# -------------------------\n",
    "def apply_mapping_tier(df_map: pd.DataFrame, df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Priority:\n",
    "      1) (programme1, level1, vertical1)\n",
    "      2) (programme1, level1)\n",
    "      3) (programme1, vertical1)\n",
    "      4) (programme1)\n",
    "    Returns name_out, code_out Series.\n",
    "    \"\"\"\n",
    "    name_out = pd.Series(pd.NA, index=df.index, dtype=\"object\")\n",
    "    code_out = pd.Series(pd.NA, index=df.index, dtype=\"object\")\n",
    "    if \"Programme Name\" not in df_map.columns:\n",
    "        return name_out, code_out\n",
    "\n",
    "    # Normalize lookup columns to lowercase\n",
    "    m = df_map.copy()\n",
    "    for k in [\"programme1\", \"level1\", \"vertical1\"]:\n",
    "        if k in m.columns:\n",
    "            m[k + \"__lc\"] = m[k].astype(str).str.lower()\n",
    "\n",
    "    # Build keyed dicts and fill in priority order with .map (fast)\n",
    "    def _map_by_keys(keys):\n",
    "        # Only proceed if all keys exist in both df and map\n",
    "        if not all((k + \"__lc\") in df.columns for k in keys):\n",
    "            return\n",
    "        if not all((k + \"__lc\") in m.columns for k in keys):\n",
    "            return\n",
    "        key_series = df[[k + \"__lc\" for k in keys]].astype(str).agg(\"|\".join, axis=1)\n",
    "        key_map = m.drop_duplicates(subset=[k + \"__lc\" for k in keys]).copy()\n",
    "        key_map[\"__k__\"] = key_map[[k + \"__lc\" for k in keys]].astype(str).agg(\"|\".join, axis=1)\n",
    "        name_dict = dict(zip(key_map[\"__k__\"], key_map[\"Programme Name\"]))\n",
    "        code_dict = dict(zip(key_map[\"__k__\"], key_map[\"Programme Code\"])) if \"Programme Code\" in key_map.columns else {}\n",
    "\n",
    "        fill_mask = name_out.isna()\n",
    "        if fill_mask.any():\n",
    "            ks = key_series.where(fill_mask)\n",
    "            name_out.update(ks.map(name_dict))\n",
    "            if code_dict:\n",
    "                code_out.update(ks.map(code_dict))\n",
    "\n",
    "    _map_by_keys([\"programme1\", \"level1\", \"vertical1\"])\n",
    "    _map_by_keys([\"programme1\", \"level1\"])\n",
    "    _map_by_keys([\"programme1\", \"vertical1\"])\n",
    "    _map_by_keys([\"programme1\"])\n",
    "\n",
    "    return name_out, code_out\n",
    "\n",
    "# -------------------------\n",
    "# 6) Apply tiers in priority order (fully vectorized)\n",
    "# -------------------------\n",
    "TIERS = [first_tier, second_tier, third_tier, fourth_tier, odl_tier]\n",
    "\n",
    "out_name = pd.Series(pd.NA, index=data_df.index, dtype=\"object\")\n",
    "out_code = pd.Series(pd.NA, index=data_df.index, dtype=\"object\")\n",
    "\n",
    "for tier_df in TIERS:\n",
    "    if \"Programme_Code_Rule\" in tier_df.columns:\n",
    "        n, c = apply_rule_tier(tier_df, data_df)\n",
    "    else:\n",
    "        n, c = apply_mapping_tier(tier_df, data_df)\n",
    "\n",
    "    fill = out_name.isna() & n.notna()\n",
    "    if fill.any():\n",
    "        out_name.loc[fill] = n.loc[fill]\n",
    "        out_code.loc[fill] = c.loc[fill]\n",
    "\n",
    "    # Early exit if all rows resolved\n",
    "    if out_name.notna().all():\n",
    "        break\n",
    "\n",
    "# Fallbacks\n",
    "out_name = out_name.fillna(\"Unknown\")\n",
    "out_code = out_code.astype(\"object\")\n",
    "\n",
    "data_df[\"programme_name\"] = out_name\n",
    "data_df[\"programme_code\"] = out_code\n",
    "\n",
    "data_df = data_df.drop(['programme1__lc', 'level1__lc','vertical1__lc'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7794d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.dispose()\n",
    "import os\n",
    "from urllib.parse import quote\n",
    "from sqlalchemy import create_engine\n",
    "from dotenv import load_dotenv\n",
    " \n",
    "def marcommdb_connection():\n",
    "    # Load environment variables\n",
    "    load_dotenv(override=True) \n",
    " \n",
    "    # Get credentials from environment variables\n",
    "    username = os.getenv(\"PG_USERNAME\")\n",
    "    password = os.getenv(\"PG_PASSWORD\")\n",
    "    host = os.getenv(\"PG_HOST\")\n",
    "    port = os.getenv(\"PG_PORT\")\n",
    "    database = os.getenv(\"PG_DATABASE_EXPORT\")\n",
    " \n",
    "    # Ensure all credentials are available\n",
    "    if not all([username, password, host, port, database]):\n",
    "        raise ValueError(\"Missing one or more PostgreSQL environment variables!\")\n",
    " \n",
    "    # Encode password to handle special characters\n",
    "    encoded_password = quote(password, safe=\"\") if password else \"\"\n",
    " \n",
    "    # Construct PostgreSQL connection string\n",
    "    DATABASE_URL = f\"postgresql+psycopg2://{username}:{encoded_password}@{host}:{port}/{database}\"\n",
    " \n",
    "    # Create and return SQLAlchemy engine\n",
    "    return create_engine(DATABASE_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a80c7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine= marcommdb_connection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7706151b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "754"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sqlalchemy.types import Integer\n",
    "\n",
    "data_df.to_sql(\n",
    "    'leads_opp_staging',\n",
    "    engine,\n",
    "    schema='staging',\n",
    "    if_exists='append',\n",
    "    index=False\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dna_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
