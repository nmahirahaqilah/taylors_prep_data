{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3464ef35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from simple_salesforce import Salesforce\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from datetime import date\n",
    "from r2r_pipelines import export_db\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc3f9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import r2r_pipelines as r2r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e062b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv  \n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aaf801f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "SF_USERNAME = 'insights@taylors.edu.my'\n",
    "SF_PASSWORD = 'Xuy35125'\n",
    "SF_SECURITY_TOKEN = 'GPzpmdXt6wjBeCBzIYZoaJi9'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dec976ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to Salesforce\n",
    "sf = Salesforce(username=SF_USERNAME, password=SF_PASSWORD, security_token=SF_SECURITY_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3a80588",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_query = sf.query_all(\"\"\"Select \n",
    "Id,\n",
    "LeadId, \n",
    "IsDeleted, \n",
    "NewValue,\n",
    "OldValue,\n",
    "CreatedDate\n",
    "from \n",
    "LeadHistory \n",
    "WHERE Field = 'Status'\n",
    "AND CreatedDate > 2022-01-01T00:00:00.000Z\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "27315915",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_sf_record(record, parent_key='', sep='_'):\n",
    "    items = []\n",
    "    for k, v in record.items():\n",
    "        new_key = f\"{parent_key}{sep}{k}\" if parent_key else k\n",
    "        if new_key in ['attributes_type', 'attributes_url']:\n",
    "            continue  # skip these keys\n",
    "        if isinstance(v, dict):\n",
    "            items.extend(flatten_sf_record(v, new_key, sep=sep).items())\n",
    "        else:\n",
    "            items.append((new_key, v))\n",
    "    return dict(items)\n",
    "\n",
    "# Flatten all Salesforce records\n",
    "flattened_records = [flatten_sf_record(rec) for rec in new_query['records']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4fd32d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lead_history = pd.DataFrame(flattened_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c6614cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>LeadId</th>\n",
       "      <th>IsDeleted</th>\n",
       "      <th>NewValue</th>\n",
       "      <th>OldValue</th>\n",
       "      <th>CreatedDate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0172j0000Hcj1NbAQI</td>\n",
       "      <td>00Q0I00000lMSW4UAO</td>\n",
       "      <td>False</td>\n",
       "      <td>Invalid</td>\n",
       "      <td>Lost</td>\n",
       "      <td>2023-03-29T00:10:46.000+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0172j0000Hcj1XlAQI</td>\n",
       "      <td>00Q0I00000lMSW4UAO</td>\n",
       "      <td>False</td>\n",
       "      <td>Invalid (3 Attempts)</td>\n",
       "      <td>Invalid</td>\n",
       "      <td>2023-03-29T00:13:55.000+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0172j0000Hcj1bRAQQ</td>\n",
       "      <td>00Q0I00000lMSW4UAO</td>\n",
       "      <td>False</td>\n",
       "      <td>Invalid</td>\n",
       "      <td>Invalid (3 Attempts)</td>\n",
       "      <td>2023-03-29T00:14:43.000+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0172j0000Hcj1XWAQY</td>\n",
       "      <td>00Q0I00000lMSWOUA4</td>\n",
       "      <td>False</td>\n",
       "      <td>Invalid (3 Attempts)</td>\n",
       "      <td>Invalid</td>\n",
       "      <td>2023-03-29T00:13:55.000+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0172j0000Hcj1bCAQQ</td>\n",
       "      <td>00Q0I00000lMSWOUA4</td>\n",
       "      <td>False</td>\n",
       "      <td>Invalid</td>\n",
       "      <td>Invalid (3 Attempts)</td>\n",
       "      <td>2023-03-29T00:14:43.000+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494617</th>\n",
       "      <td>017Mg00007UB4uWIAT</td>\n",
       "      <td>00QMg00000D9dfNMAR</td>\n",
       "      <td>False</td>\n",
       "      <td>Qualified Converted</td>\n",
       "      <td>Warm</td>\n",
       "      <td>2025-08-06T05:54:16.000+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494618</th>\n",
       "      <td>017Mg00007UCOfyIAH</td>\n",
       "      <td>00QMg00000D9j4rMAB</td>\n",
       "      <td>False</td>\n",
       "      <td>Hot</td>\n",
       "      <td>New</td>\n",
       "      <td>2025-08-06T06:10:06.000+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494619</th>\n",
       "      <td>017Mg00007UHUU1IAP</td>\n",
       "      <td>00QMg00000D9oFqMAJ</td>\n",
       "      <td>False</td>\n",
       "      <td>Warm</td>\n",
       "      <td>New</td>\n",
       "      <td>2025-08-06T06:50:43.000+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494620</th>\n",
       "      <td>017Mg00007UHYHcIAP</td>\n",
       "      <td>00QMg00000D9oFqMAJ</td>\n",
       "      <td>False</td>\n",
       "      <td>Qualified Converted</td>\n",
       "      <td>Warm</td>\n",
       "      <td>2025-08-06T06:53:56.000+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494621</th>\n",
       "      <td>017Mg00007UHHApIAP</td>\n",
       "      <td>00QMg00000D9rDhMAJ</td>\n",
       "      <td>False</td>\n",
       "      <td>Qualified Converted</td>\n",
       "      <td>New</td>\n",
       "      <td>2025-08-06T06:56:57.000+0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>494622 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Id              LeadId  IsDeleted  \\\n",
       "0       0172j0000Hcj1NbAQI  00Q0I00000lMSW4UAO      False   \n",
       "1       0172j0000Hcj1XlAQI  00Q0I00000lMSW4UAO      False   \n",
       "2       0172j0000Hcj1bRAQQ  00Q0I00000lMSW4UAO      False   \n",
       "3       0172j0000Hcj1XWAQY  00Q0I00000lMSWOUA4      False   \n",
       "4       0172j0000Hcj1bCAQQ  00Q0I00000lMSWOUA4      False   \n",
       "...                    ...                 ...        ...   \n",
       "494617  017Mg00007UB4uWIAT  00QMg00000D9dfNMAR      False   \n",
       "494618  017Mg00007UCOfyIAH  00QMg00000D9j4rMAB      False   \n",
       "494619  017Mg00007UHUU1IAP  00QMg00000D9oFqMAJ      False   \n",
       "494620  017Mg00007UHYHcIAP  00QMg00000D9oFqMAJ      False   \n",
       "494621  017Mg00007UHHApIAP  00QMg00000D9rDhMAJ      False   \n",
       "\n",
       "                    NewValue              OldValue  \\\n",
       "0                    Invalid                  Lost   \n",
       "1       Invalid (3 Attempts)               Invalid   \n",
       "2                    Invalid  Invalid (3 Attempts)   \n",
       "3       Invalid (3 Attempts)               Invalid   \n",
       "4                    Invalid  Invalid (3 Attempts)   \n",
       "...                      ...                   ...   \n",
       "494617   Qualified Converted                  Warm   \n",
       "494618                   Hot                   New   \n",
       "494619                  Warm                   New   \n",
       "494620   Qualified Converted                  Warm   \n",
       "494621   Qualified Converted                   New   \n",
       "\n",
       "                         CreatedDate  \n",
       "0       2023-03-29T00:10:46.000+0000  \n",
       "1       2023-03-29T00:13:55.000+0000  \n",
       "2       2023-03-29T00:14:43.000+0000  \n",
       "3       2023-03-29T00:13:55.000+0000  \n",
       "4       2023-03-29T00:14:43.000+0000  \n",
       "...                              ...  \n",
       "494617  2025-08-06T05:54:16.000+0000  \n",
       "494618  2025-08-06T06:10:06.000+0000  \n",
       "494619  2025-08-06T06:50:43.000+0000  \n",
       "494620  2025-08-06T06:53:56.000+0000  \n",
       "494621  2025-08-06T06:56:57.000+0000  \n",
       "\n",
       "[494622 rows x 6 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lead_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bebec38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "lead_history['CreatedDate'] = lead_history['CreatedDate'].apply(\n",
    "    lambda x: datetime.strptime(x, '%Y-%m-%dT%H:%M:%S.000%z').date() if pd.notnull(x) else None\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d14cf4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "second_query = sf.query_all(\"\"\"Select \n",
    "Id,\n",
    "CreatedDate, \n",
    "LeadSource, \n",
    "Lead_status__c, \n",
    "ConvertedAccountId, \n",
    "ConvertedOpportunityId, \n",
    "ConvertedDate, \n",
    "CYCLE__c,\n",
    "Intake_Year__c                            \n",
    "from \n",
    "Lead\n",
    "where Intake_Year__c in ('2022','2023','2024','2025','2026') or CreatedDate > 2022-01-01T00:00:00.000Z\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53967a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_sf_record(record, parent_key='', sep='_'):\n",
    "    items = []\n",
    "    for k, v in record.items():\n",
    "        new_key = f\"{parent_key}{sep}{k}\" if parent_key else k\n",
    "        if new_key in ['attributes_type', 'attributes_url']:\n",
    "            continue  # skip these keys\n",
    "        if isinstance(v, dict):\n",
    "            items.extend(flatten_sf_record(v, new_key, sep=sep).items())\n",
    "        else:\n",
    "            items.append((new_key, v))\n",
    "    return dict(items)\n",
    "\n",
    "# Flatten all Salesforce records\n",
    "flattened_records = [flatten_sf_record(rec) for rec in second_query['records']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ea2e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "leads = pd.DataFrame(flattened_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c5be82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "leads['CreatedDate'] = leads['CreatedDate'].apply(\n",
    "    lambda x: datetime.strptime(x, '%Y-%m-%dT%H:%M:%S.000%z').date() if pd.notnull(x) else None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76345989",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe82b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "leads=leads.rename(columns={\n",
    "    'Id':'lead_id',\n",
    "    'CreatedDate':'validfrom',\n",
    "    'Lead_status__c':'lead_status',\n",
    "    'CYCLE__c':'cycle',\n",
    "    'Intake_Year__c':'intake_year',\n",
    "    'LeadSource':'lead_source',\n",
    "    'ConvertedAccountId':'converted_account_id',\n",
    "    'ConvertedOpportunityId':'converted_opportunity_id',\n",
    "    'ConvertedDate':'converted_date'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a60613",
   "metadata": {},
   "outputs": [],
   "source": [
    "lead_history=lead_history.rename(columns={\n",
    "    'Id':'lead_history_id',\n",
    "    'LeadId':'lead_id',\n",
    "    'IsDeleted':'isdeleted',\n",
    "    'NewValue':'new_value',\n",
    "    'OldValue':'old_value',\n",
    "    'CreatedDate':'validto'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859a0662",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merged_df = pd.merge(filtered_df, filtered_df_2, left_on='leadid', right_on='lead_id', how='left')\n",
    "merged_df = pd.merge(leads, lead_history, left_on='lead_id', right_on='lead_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29dab233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Date Conversion with Error Handling ---\n",
    "merged_df['validfrom'] = pd.to_datetime(merged_df['validfrom'], format='%d/%m/%Y', errors='coerce')\n",
    "merged_df['validto'] = pd.to_datetime(merged_df['validto'], format='%d/%m/%Y', errors='coerce')\n",
    "merged_df['converted_date'] = pd.to_datetime(merged_df['converted_date'], format='%Y-%m-%d', errors='coerce')\n",
    "\n",
    "# --- DIAGNOSTIC: Check for NaT values after conversion ---\n",
    "print(\"--- NaT Value Check (after conversion) ---\")\n",
    "print(f\"NaT values in 'validfrom': {merged_df['validfrom'].isna().sum()}\")\n",
    "print(f\"NaT values in 'validto': {merged_df['validto'].isna().sum()}\")\n",
    "print(f\"NaT values in 'converted_date': {merged_df['converted_date'].isna().sum()}\")\n",
    "print(\"-------------------------------------------\\n\")\n",
    "\n",
    "# Filter out rows with invalid dates\n",
    "merged_df.dropna(subset=['validfrom', 'validto'], inplace=True)\n",
    "if merged_df.empty:\n",
    "    print(\"Warning: merged_df is empty after dropping rows with invalid dates.\")\n",
    "    exit()\n",
    "\n",
    "# --- 2. Construct the SCD Type 2 table records ---\n",
    "df_history_sorted = merged_df.sort_values(by=['lead_id', 'validfrom', 'validto']).copy()\n",
    "scd_rows = []\n",
    "\n",
    "common_attrs = [\n",
    "    'lead_source', 'converted_account_id', 'converted_opportunity_id',\n",
    "    'cycle', 'converted_date', 'intake_year'\n",
    "]\n",
    "\n",
    "for lead_id, group in df_history_sorted.groupby('lead_id'):\n",
    "    first_event = group.iloc[0]\n",
    "    \n",
    "    # Handle initial 'old_value' record\n",
    "    initial_status = first_event['old_value']\n",
    "    initial_validto = first_event['validto']\n",
    "    \n",
    "    # SPECIAL CASE: If status is \"New\", use validfrom as modifieddate\n",
    "    if initial_status == 'New':\n",
    "        initial_modifieddate = first_event['validfrom']\n",
    "    else:\n",
    "        initial_modifieddate = pd.NaT\n",
    "    \n",
    "    initial_row = {\n",
    "        'lead_id': lead_id,\n",
    "        'status': initial_status,\n",
    "        'modifieddate': initial_modifieddate,\n",
    "        'validto': initial_validto,\n",
    "    }\n",
    "    for attr in common_attrs:\n",
    "        initial_row[attr] = first_event[attr]\n",
    "    scd_rows.append(initial_row)\n",
    "    \n",
    "    last_validto = initial_validto\n",
    "\n",
    "    # Process each 'new_value' event\n",
    "    for _, record in group.iterrows():\n",
    "        current_status = record['new_value']\n",
    "        current_validto = record['validto']\n",
    "        \n",
    "        # SPECIAL CASE: If status is \"New\", use validfrom as modifieddate\n",
    "        if current_status == 'New':\n",
    "            current_modifieddate = record['validfrom']\n",
    "        else:\n",
    "            current_modifieddate = last_validto\n",
    "        \n",
    "        current_row = {\n",
    "            'lead_id': lead_id,\n",
    "            'status': current_status,\n",
    "            'modifieddate': current_modifieddate,\n",
    "            'validto': current_validto,\n",
    "        }\n",
    "        for attr in common_attrs:\n",
    "            current_row[attr] = record[attr]\n",
    "        scd_rows.append(current_row)\n",
    "        \n",
    "        last_validto = current_validto\n",
    "\n",
    "# Create raw SCD DataFrame\n",
    "scd_df_raw = pd.DataFrame(scd_rows)\n",
    "scd_df_raw = scd_df_raw.sort_values(by=['lead_id', 'modifieddate']).reset_index(drop=True)\n",
    "\n",
    "# --- 3. Collapse Contiguous Same-Status Periods ---\n",
    "scd_df_raw['prev_status'] = scd_df_raw.groupby('lead_id')['status'].shift(1)\n",
    "scd_df_raw['prev_validto'] = scd_df_raw.groupby('lead_id')['validto'].shift(1)\n",
    "\n",
    "scd_df_raw['new_record_flag'] = (\n",
    "    (scd_df_raw['status'] != scd_df_raw['prev_status']) |\n",
    "    (scd_df_raw['modifieddate'] != scd_df_raw['prev_validto']) |\n",
    "    (scd_df_raw['prev_status'].isna()) |\n",
    "    (scd_df_raw['modifieddate'].isna())\n",
    ")\n",
    "\n",
    "scd_df_raw['group_id'] = scd_df_raw.groupby('lead_id')['new_record_flag'].cumsum()\n",
    "\n",
    "final_scd_df = scd_df_raw.groupby(['lead_id', 'group_id']).agg(\n",
    "    status=('status', 'first'),\n",
    "    modifieddate=('modifieddate', lambda x: x.iloc[0] if pd.notna(x.iloc[0]) else pd.NaT),\n",
    "    validto=('validto', 'max'),\n",
    "    lead_source=('lead_source', 'first'),\n",
    "    converted_account_id=('converted_account_id', 'first'),\n",
    "    converted_opportunity_id=('converted_opportunity_id', 'first'),\n",
    "    cycle=('cycle', 'first'),\n",
    "    converted_date=('converted_date', 'first'),\n",
    "    intake_year=('intake_year', 'first')\n",
    ").reset_index()\n",
    "\n",
    "final_scd_df.drop(columns=['group_id'], inplace=True)\n",
    "\n",
    "# --- 4. Determine 'current_flag' ---\n",
    "MAX_OBSERVED_END_DATE = merged_df['validto'].max()\n",
    "max_validto_per_lead = final_scd_df.groupby('lead_id')['validto'].transform('max')\n",
    "\n",
    "final_scd_df['current_flag'] = (\n",
    "    (final_scd_df['validto'] == max_validto_per_lead) &\n",
    "    (final_scd_df['validto'] == MAX_OBSERVED_END_DATE)\n",
    ").astype(int)\n",
    "\n",
    "# --- 5. Format final output ---\n",
    "column_order = [\n",
    "    'lead_id', 'status', 'modifieddate', 'validto', 'lead_source',\n",
    "    'converted_account_id', 'converted_opportunity_id', 'cycle',\n",
    "    'converted_date', 'intake_year', 'current_flag'\n",
    "]\n",
    "\n",
    "final_scd_df = final_scd_df[column_order]\n",
    "\n",
    "# Format dates\n",
    "# for col in ['modifieddate', 'validto', 'converted_date']:\n",
    "#    if col in final_scd_df.columns and pd.api.types.is_datetime64_any_dtype(final_scd_df[col]):\n",
    "#        final_scd_df[col] = final_scd_df[col].dt.strftime('%d/%m/%Y').replace('NaT', 'NaN')\n",
    "\n",
    "for col in ['modifieddate', 'validto', 'converted_date']:\n",
    "    if col in final_scd_df.columns:\n",
    "        final_scd_df[col] = pd.to_datetime(final_scd_df[col], errors='coerce') \\\n",
    "                                .dt.strftime('%d/%m/%Y')\n",
    "\n",
    "print(\"\\n--- Final SCD Type 2 Lead Status History ---\")\n",
    "print(final_scd_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a4bcb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_scd_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04240beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_scd_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a2e121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert specific columns to datetime\n",
    "columns_to_convert = ['modifieddate', 'validto', 'converted_date']\n",
    "\n",
    "for col in columns_to_convert:\n",
    "    final_scd_df[col] = pd.to_datetime(final_scd_df[col], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623e70a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_scd_df=final_scd_df.rename(columns={\n",
    "    'status':'lead_status'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125c1f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy.types import Date\n",
    "\n",
    "engine = export_db.marcommdb_connection()\n",
    "final_scd_df.to_sql('leads_history_staging', engine, schema='staging', if_exists='replace', index=False,dtype={'modifieddate': Date, 'validto': Date, 'converted_date': Date})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f957bac6",
   "metadata": {},
   "source": [
    "### Leads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e12822",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('c:/Users/112363/dna_pipeline/dna_pipelines')  # Adjust this to your actual path\n",
    "\n",
    "from r2r_pipelines import export_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f442d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from simple_salesforce import Salesforce\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from datetime import date\n",
    "from r2r_pipelines import export_db\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bf274b",
   "metadata": {},
   "outputs": [],
   "source": [
    "SF_USERNAME = 'insights@taylors.edu.my'\n",
    "SF_PASSWORD = 'Xuy35125'\n",
    "SF_SECURITY_TOKEN = 'GPzpmdXt6wjBeCBzIYZoaJi9'\n",
    "\n",
    "# Connect to Salesforce\n",
    "sf = Salesforce(username=SF_USERNAME, password=SF_PASSWORD, security_token=SF_SECURITY_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25632e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "leads_query = sf.query_all(\"\"\"Select \n",
    "Id,\n",
    "CreatedDate, \n",
    "LeadSource, \n",
    "Lead_status__c, \n",
    "ConvertedAccountId, \n",
    "ConvertedOpportunityId, \n",
    "ConvertedDate, \n",
    "CYCLE__c,\n",
    "Intake_Year__c                            \n",
    "from \n",
    "Lead\n",
    "where Intake_Year__c in ('2022','2023','2024','2025','2026') or CreatedDate > 2022-01-01T00:00:00.000Z\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2722ce0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_sf_record(record, parent_key='', sep='_'):\n",
    "    items = []\n",
    "    for k, v in record.items():\n",
    "        new_key = f\"{parent_key}{sep}{k}\" if parent_key else k\n",
    "        if new_key in ['attributes_type', 'attributes_url']:\n",
    "            continue  # skip these keys\n",
    "        if isinstance(v, dict):\n",
    "            items.extend(flatten_sf_record(v, new_key, sep=sep).items())\n",
    "        else:\n",
    "            items.append((new_key, v))\n",
    "    return dict(items)\n",
    "\n",
    "# Flatten all Salesforce records\n",
    "flattened_records = [flatten_sf_record(rec) for rec in leads_query['records']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3cdda03",
   "metadata": {},
   "outputs": [],
   "source": [
    "leads = pd.DataFrame(flattened_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29500d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "leads.rename(columns={'Id': 'lead_id','CreatedDate':'created_date','Lead_status__c':'lead_status',\n",
    "                      'ConvertedAccountId':'converted_account_id','ConvertedOpportunityId':'converted_opportunity_id',\n",
    "                      'ConvertedDate':'converted_date','CYCLE__c':'cycle','Intake_Year__c':'intake_year'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200fa082",
   "metadata": {},
   "outputs": [],
   "source": [
    "leads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44d0cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "leads['created_date'] = leads['created_date'].apply(\n",
    "    lambda x: datetime.strptime(x, '%Y-%m-%dT%H:%M:%S.000%z').date() if pd.notnull(x) else None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4e9fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy.types import Date\n",
    "engine = export_db.marcommdb_connection()\n",
    "\n",
    "leads.to_sql(\n",
    "    'leads_staging',\n",
    "    engine,\n",
    "    schema='staging',\n",
    "    if_exists='replace',\n",
    "    index=False,\n",
    "    dtype={'created_date': Date,'converted_date': Date}\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dna_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
